```scala
abstract class RDD[T: ClassTag](
    @transient private var _sc: SparkContext,
    @transient private var deps: Seq[Dependency[_]]
  ) extends Serializable with Logging {
    // 计算分区的函数
    def compute(split: Partition, context: TaskContext) : Iterator[T]
    // 一组分片
    protected def getPartitions: Array[Partition]
    // RDD间依赖关系
    protected def getDependencies: Seq[Dependency[_]] = deps
    // 每个分区的优先位置
    protected def getPreferredLocations(split: Partition): Seq[String] = Nil
    // RDD分片函数
    @transient val partitioner: Option[Partitioner] = None

    // 标记一个要被持久化的RDD, 一旦首次被触发
    // 该RDD将会被保留在计算节点的内存中并重用
    // cache()是使用persist()的快捷方法
    def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)
    def cache(): this.type = persist()

    // 指定RDD需要检查点机制
    def checkpoint()
}
```

# 依赖(Depencency)

RDD和它依赖的parent RDD(s)的关系有两种不同的类型，即窄依赖(narrow dependency)和宽依赖(wide dependency)。
1. 窄依赖指的是每一个parent RDD的Partition最多被子RDD的一个Partition使用
2. 宽依赖指的是多个子RDD的Partition会依赖同一个parent RDD的Partition

```plantuml
@startuml
abstract class Dependency {}
abstract class NarrowDependency {}

class OneToOneDependency {}
note top : 一对一的依赖

class RangeDependency {}
note top : 范围依赖，仅被UnionRDD使用

class ShuffleDependency {

}
note top : 宽依赖，子RDD依赖于Parent RDD\n所有分区需要Shuffle

NarrowDependency -down-|> Dependency
RangeDependency -down-|> NarrowDependency
OneToOneDependency -down-|> NarrowDependency
ShuffleDependency -down-|> Dependency
@enduml
```

`Dependency`是依赖表达的基类，在其上派生出`NarrowDependency`表达窄依赖和`ShuffleDependency`表示宽依赖。
```scala
abstract class Dependency[T] extends Serializable {
  // 依赖的Parent的RDD
  def rdd: RDD[T]
}
```

```scala
// 窄依赖
abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] {
  def getParents(partitionId: Int): Seq[Int]

  override def rdd: RDD[T] = _rdd
}
```

有两种窄依赖的具体实现，一种是一对一的依赖，即`OneToOneDependency`和范围的依赖，即`RangeDependency`，它仅仅被`org.apache.spark.rdd.UnionRDD`使用。UnionRDD是把多个RDD合成一个RDD，这些RDD是被拼接而成，即每个parent RDD的Partition的相对顺序不会变，只不过每个parent RDD在UnionRDD中的Partition的起始位置不同.

```scala
class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) {
  override def getParents(partitionId: Int): List[Int] = List(partitionId)
}

class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int)
  extends NarrowDependency[T](rdd) {
  override def getParents(partitionId: Int): List[Int] = {
    if (partitionId >= outStart && partitionId < outStart + length) {
      List(partitionId - outStart + inStart)
    } else {
      Nil
    }
  }
}
```

宽依赖的实现只有一种：ShuffleDependency。子RDD依赖于parent RDD的所有Partition，需要Shuffle过程
```scala
class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](
    @transient private val _rdd: RDD[_ <: Product2[K, V]],
    val partitioner: Partitioner,
    val serializer: Serializer = SparkEnv.get.serializer,
    val keyOrdering: Option[Ordering[K]] = None,
    val aggregator: Option[Aggregator[K, V, C]] = None,
    val mapSideCombine: Boolean = false,
    val shuffleWriterProcessor: ShuffleWriteProcessor = 
        new ShuffleWriteProcessor)
  extends Dependency[Product2[K, V]] with Logging {
            ......
  // 获取新的shuffle id
  val shuffleId: Int = _rdd.context.newShuffleId()

  // 向ShuffleManager注册Shuffle的信息
  val shuffleHandle: ShuffleHandle =
    _rdd.context.env.shuffleManager.registerShuffle(
        shuffleId, this)
    
        ......
}
```

宽依赖支持两种Shuffle Manager，即`org.apache.spark.shuffle.hash.HashShuffleManager`(基于Hash的Shuffle机制)和`org.apache.spark.shuffle.sort.SortShuffleManager`(基于排序的Shuffle机制）

