在Doris诞生之前，百度和大多数互联网公司一样，使用MySQL的Sharding为OLAP报表业务提供支持。在早些年，百度的主要收入来源是广告，广告主需要通过报表查看广告的展现效果、点击量、收入等信息，并且根据不同维度分析制定后续广告的投放策略。随着百度本身流量的增加，广告流量也随之增加，MySQL的Sharding方案无法满足业务需求，主要痛点如下。
- 大规模数据导入会导致MySQL的读性能大幅降低，有时还会出现锁表现象，导致查询超时，尤其在频繁导入数据时，问题更为明显。
- MySQL在数据量达千万级别时性能很差，只能从产品层面来限制用户的查询时间，抑制了用户需求，增加了很多后台取数的需求。
- MySQL单表存储的数据有限，如果数据量过大，查询就会变慢。而且随着数据量的快速增长，Sharding方案维护成本飙升。

<b><font color=FF7D33>
默认的行列混合存储、向量化执行引擎、改进的MPP架构
Doris支持标准的SQL并且完全兼容MySQL协议，仅需亚秒级响应时间即可返回海量数据下的查询结果。

Doris中的谓词下推不仅可以穿透查询层，可以下推到存储层，利用索引进行数据过滤。

Doris从设计之初就一直以易用性为出发点。

- 在数据建模方面，Doris支持Aggregate、Unique和Duplicate三种模型，可以满足OLAP领域的各种应用场景。
- 在数据导入方面支持多种数据导入方案，同时在数据导入过程中保持原子性，在数据消费侧可以实现At-Most-Once语义。

集群可靠性方面，Doris使用内存存储+检查点+镜像日志文件模式，使用BTBJE(类似于Raft)协议实现元数据的高可用和高可靠。

集群扩缩容方面，Doris基于分布式管理框架，自动管理数据副本分布、修复和均衡，比如副本损坏，Doris会自动感知并修复。对于节点扩缩容，Doris自动进行数据分片均衡，整个过程完全不影响其他服务，无需运维人员进行操作。

集群升级方面，在设计上完全向前兼容，支持通过灰度发布方式进行新版本的验证和测试。

Doris在0.15版本中引入向量化执行引擎，在1.0版本中逐渐成熟

</font>
</b>

---

在Palo 2版本中，元数据管理、分片管理和元数据副本管理合并到Frontend（Doris前端模块，以下简称为FE）组件，OLAP存储引擎和查询引擎合并到Backend（Doris后端模块，以下简称为BE）组件，减少了Agent组件和uWSGI服务组件。FE负责接收用户的查询请求，对查询进行规划，监督查询执行情况，并将查询结果返给用户。BE负责数据的存储，维护多副本数据，以及具体的查询执行。

经过Palo 2版本的改进，Doris的架构变得相当简洁，并且不再需要任何外部依赖。在此之后，虽然Doris经过几次改进，但是整体架构仍然保持Palo 2的架构。

---
# 架构

从设计上来说，Doris融合了Google Mesa的数据存储模型、Apache的ORCFile存储格式、Apache Impala的查询引擎和MySQL交互协议.

```
Doris = Google Mesa     +   Apache Impala     +     Apache ORCFile
              |                 |                       |
        聚合数据模型          查询引擎                文件格式
        数据版本管理                                   索引
        前缀索引                                    压缩和编码
        在线模式变更
```

在架构方面，Doris只有两类进程：一类是FE，可以理解为Doris的管理节点，主要负责用户请求的接入、查询计划的解析、元数据的存储和集群管理相关工作；另一类是BE，主要负责数据存储、查询计划执行。

FE节点包含Leader、Follower和Observer三种角色。默认一个集群只能有一个Leader，可以有多个Follower和Observer。其中，Leader和Follower组成一个Paxos选择组，如果Leader宕机，剩下的Follower会自动选出新的Leader，保证写入高可用。Observer同步Leader的数据，但是不参加选举。如果只部署一个FE，FE默认就是Leader。

<b>FE节点主要包含存储管理模块、状态管理模块、协调模块、元数据模块和元数据缓存模块。</b>存储管理模块负责管理所有的元数据信息，包括表信息、Tablet信息、Tablet的副本信息等，还负责管理用户的权限信息（即用户的认证信息和授权信息）和数据的导入任务等。状态管理模块负责管理所有BE进程的存活状态、查询负载等非持久化信息，并提供发布订阅接口。协调模块负责接收用户发来的请求，然后进行语句解析、生成执行规划，根据当前集群的状态，对执行规划进行调度。元数据模块负责对元数据的读写，只有Leader角色拥有此权限。元数据缓存模块负责同步元数据，以供语句解析、生成执行规划，主要是Follower和Observer角色拥有此权限。

BE节点可以无限扩展，并且所有BE节点的角色都是对等的。在集群足够大的情况下，部分BE节点下线不影响集群提供服务。BE节点主要由存储引擎和查询执行器组成。存储引擎负责管理节点本地的Tablet数据，发送或者接收数据并保存副本，定期合并、更新多个版本的数据，以减少存储占用。存储引擎还负责接收来自查询执行器的数据读取请求和批量数据导入请求。

<b><font color=FF4533>
FE的状态管理模块负责管理BE进程存活状态，如果BE节点挂了如何处理？
BE节点如何做到无限扩展？
</font></b>

在数据导入方面支持多种数据导入方案，同时在数据导入过程中保持原子性。不论使用Broker Load进行批量导入，还是使用INSERT语句进行单条导入，都是一个完整的事务操作。导入事务可以保证一个批次内的数据原子性生效，不会出现部分数据写入的情况。每一个导入作业都会生成一个Label，这个Label在数据库内用于唯一区分导入作业。Label用于保证对应的导入作业仅能成功导入一次，一个成功导入的Label再次调用时，会被拒绝并报错：Label already used。通过这个机制，数据消费侧可以实现At-Most-Once语义。如果结合上游系统的At-Least-Once语义，可以实现端到端数据导入的Exactly-Once语义。

# 特色功能

## 分区分桶裁剪功能

Doris支持两个层次的数据划分：第一层是分区(Partition)，支持Range和List的划分方式；第二层是分桶(Bucket)，将数据通过Hash值进行水平划分，数据分片(Tablet)在集群中被均匀打散。利用分桶裁剪功能，Doris可以将查询固定到极少数分片上，从而有效降低单个查询对系统资源的消耗，提升集群整体的并发查询能力。在高并发查询场景中，Doris单节点可以支持每秒上千的查询请求。

## 合理的缓存功能

Doris支持SQL级别和Partition级别的查询缓存。其中，SQL级别的缓存以SQL语句的Hash值为Key，直接缓存SQL查询结果，非常适合更新频率不高，但是查询非常频繁的场景。而Partition级别的缓存会智能地将SQL查询结果中不同分区的数据进行存储，之后的查询中可以利用已缓存分区的数据及新分区实时查询数据得到最终结果，从而降低重复数据查询，减少对系统资源的消耗。

## 支持Bitmap数据类型

利用位图来存储整型数据，并且可以通过位图进行一些集合类操作，Bitmap可以应用于高基数精确去重场景。

## 物化视图

物化视图是将预先计算好的数据集存储在一个对用户透明且有真实数据的视图表格中。物化视图主要是为了满足用户对原始明细数据任意维度分析，快速对固定维度进行分析、查询，在统一视角下对明细、聚合数据进行分析的需求。在Doris中，用户可以使用明细数据模型存储明细数据，之后在明细数据上，选择任意维度和指标创建聚合物化视图，如SUM、MIN、MAX、COUNT等。Doris会保证明细表和物化视图中的数据完全一致。如果导入或删除物理表中的数据，Doris会自动更新，保证原始表和物化视图中的数据一致。同时，物化视图对用户是透明的。Doris会自动根据查询语句，匹配到最合适的物化视图进行查询。通过物化视图功能，Doris支持在一张表中统一明细数据模型和聚合模型，以加速某些固定模式的查询。

## 支持基于主键的数据更新

通过Unique模型，用户可以对数据基于主键进行更新。在实现层面，Doris采用Merge-on-Read方式提供更新后的数据。此外，用户还可以使用REPLACE_IF_NOT_NULL聚合方式，实现部分列更新。基于Unique模型，Doris还支持通过Marked Delete和Sequence Column等功能，实现对上游交易数据库数据同步更新，并且保证事务的原子性以及数据同步的顺序性。

# 查询引擎

## 查询优化器

自适应聚合，
Runtime Filter，Doris设计了不同类型的RuntimeFilter(In Predicate、Bloom Filter和MinMax)，它适用于大部分Join场景，包括节点的自动穿透，可将过滤器下推到最底层的扫描节点，例如分布式Shuffle Join中，可先将多个节点产生的过滤器进行合并，再下推到数据读取节点。

Colocation Join可以利用数据的分布情况，将原本需要去重后才能进行关联的数据，在本地完成关联，从而避免去重时大量的数据传输，它需要用户在建表时就指定表的分布，以保证需要关联查询的若干表有相同的数据分布。

Bucket Join是Colocation Join的通用版本。Colocation Join需要用户在建表时就指定表的分布，以保证需要关联查询的若干表有相同的数据分布。而Bucket Join会更智能地判断SQL中关联条件和数据分布之间的关系，将原本需要同时去重左右两张表中数据的操作，变成将右表数据重分布到左表所在节点，从而减少数据的移动

# 向量化执行引擎
传统的数据库都是典型的迭代模型，执行计划中的每个算子通过调用下一个算子的next()方法来获取数据，从最底层的数据块中一条一条读取数据，最终返给用户。它的问题在于每个Tuple都要调用一次函数，开销太大，而且因为CPU每次只处理一条数据，无法利用CPU技术升级带来的新特性，比如SIMD。向量化模型每次处理的是一批数据，这些数据会被保存在一种叫作向量的数据结构里，由于每次处理的是一批数据，因此可以在每个Batch内做各种优化。简单地说，向量化执行引擎=高效的向量数据结构(Vector) + 批量化处理模型(nextBatch) + Batch内性能优化(例如SIMD等)。

在绝大多数场景中，用户只需要将Session变量enable_vectorized_engine设置为true，FE节点在进行查询规划时就会默认将SQL算子与SQL表达式转换为向量化的执行计划，从而提升SQL执行性能。

# 参考资料

1. Doris实时数仓实战
