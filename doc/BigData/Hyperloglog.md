**基数计数(cardinality counting)**通常用来统计一个集合中不重复的元素个数，例如统计某个网站的UV，或者用户搜索网站的关键词数量。数据分析、网络监控及数据库优化等领域都会涉及到基数计数的需求。 要实现基数计数，最简单的做法是记录集合中所有不重复的元素集合$S_u$，当新来一个元素$x_i$，若中$S_u$不包含元素$x_i$，则将$x_i$加入$S_u$，否则不加入，计数值就是$S_u$的元素数量。这种做法存在两个问题：

1. 当统计的数据量变大时，相应的存储内存也会线性增长
2. 当集合$S_u$变大，判断其是否包含新加入元素$x_i$ 的成本变大

目前还没有发现更好的在大数据场景中准确计算基数的高效算法，因此在不追求绝对准确的情况下，使用概率算法算是一个不错的解决方案。概率算法不直接存储数据集合本身，通过一定的概率统计方法预估基数值，这种方法可以大大节省内存，同时保证误差控制在一定范围内。目前用于基数计数的概率算法包括:

- **Linear Counting(LC)**：早期的基数估计算法，LC在空间复杂度方面并不算优秀，实际上LC的空间复杂度与简单bitmap方法是一样的(但是有个常数项级别的降低)，都是O(Nmax)；

- **LogLog Counting(LLC)**：LogLog Counting相比于LC更加节省内存，空间复杂度只有`O(log2(log2(Nmax)))`

- **HyperLogLog Counting(HLL)**：HyperLogLog Counting是基于LLC的优化和改进，在同样空间复杂度情况下，能够比LLC的基数估计误差更小。

# N次伯努利过程(算法来源)

设a为待估集合（哈希后）中的一个元素，由上面对H的定义可知，a可以看做一个长度固定的比特串（也就是a的二进制表示），设H哈希后的结果长度为L比特，我们将这L个比特位从左到右分别编号为1、2、…、L：

```
|-----------------------------------------------------------
|	0	|	0	|	1	|	1	|	...	|	0	|	1	|
|-----------------------------------------------------------
	1		2		3		4				L-1		L
```

是从服从均与分布的样本空间中随机抽取的一个样本，因此a每个比特位服从如下分布且相互独立，也就是说就是a的每个比特位为0和1的概率各为0.5，且相互之间是独立的。
$$
\begin{array}{l}
p(x=k) &=& \left\{\begin{matrix} 
  0.5 && k = 0 \\  
  0.5 && k = 1 
\end{matrix}\right. 
\end{array}
$$


设$\rho(a)$为a的比特串中第一个1出现的位置，显然$1 \le \rho(a) \le L$，忽略比特串全为0的情况（概率为 $\frac{1}{2^L} $ ）。如果我们遍历集合中所有元素的比特串，取$\rho_{max}$为所有$\rho(a)$的最大值。此时可以将$2^{\rho_{max}}$ 作为基数的一个粗糙估计，即：
$$
\begin{array}{l}
\hat{n} = 2^{\rho_{max}} 
\end{array}
$$

# LLC

LLC采用了分桶平均的思想来消减误差。具体来说，就是将哈希空间平均分成m份，每份称之为一个桶(bucket)。对于每一个元素，其哈希值的前k比特作为桶编号，其中$2^k = m$，而后L-k个比特作为真正用于基数估计的比特串。桶编号相同的元素被分配到同一个桶，基数估计时，首先计算每个桶内元素最大的第一个1的位置，设为`M[i]`，然后对这m个值取平均后再进行估计，即：
$$
\begin{array}{l}
\hat{n} = 2^{1/m}\sum M[i]
\end{array}
$$
相当于物理试验中经常使用的多次试验取平均的做法，可以有效消减因偶然性带来的误差。



误差分析
$$
\begin{array}{l}
StdError(\hat{n}/n) \approx \frac{1.30}{\sqrt{m}} 
\end{array}
$$


# HHL

HLLC也是渐进无偏差估计，其渐进偏差标准差为：
$$
\begin{array}{l}
StdError(\hat{n}/n) \approx \frac{1.04}{\sqrt{m}} 
\end{array}
$$


# 结论

这些基数估计算法的一个好处是<b>很容易并行化</b>。对于相同分桶数和相同哈希函数，多台机器节点可以独立并行的执行这个算法；最后只要将各个节点计算的同一个桶的最大值做一个简单的合并就可以得到这个桶最终的值。而且这种并行计算的结果和单机计算结果是完全一致的，所需的额外消耗仅仅是小于1K的字节在不同节点间的传输。

基数估计算法使用很少的资源给出数据集基数的一个良好估计，一般只要使用少于1K的空间存储状态。这个方法和数据本身的特征无关，而且可以高效的进行分布式并行计算。估计结果可以用于很多方面，例如流量监控（多少不同IP访问过一个服务器）以及数据库查询优化（例如我们是否需要排序和合并，或者是否需要构建哈希表）。

# 参考资料

1. [HyperLogLog](https://www.yuque.com/abser/blog/mrv5ke)

