# 计算广告:逻辑回归

牛顿法(Newton Methods)是在当前权重向量$W$下，利用二阶泰勒展开函数作为近似目标函数，然后利用该近似函数求解目标函数下降方向：

$$
\begin{array}{l}
D_t = -{B_t}^{-1}\bigtriangledown_{W}f(W_t)
\end{array}
$$
其中，${B_t}$是目标函数在$W_t$处海森矩阵，称这个搜索方向为牛顿方向。牛顿法具有收敛速度快的优势，但是计算海森矩阵的逆矩阵比较复杂。
- 拟牛顿法(Quasi-Newton Methods)不直接计算海森矩阵或其逆矩阵，只需要每步迭代计算目标函数梯度，通过正定矩阵来拟合海森矩阵的逆矩阵，这样简化了计算过程。
- BFGS是最流行的拟牛顿法，只需通过增量计算来逼近海森矩阵的逆矩阵，避免了每次迭代的矩阵求逆运算。
- L-BFGS(Limited Memory-BFGS)是一种使用限量内存的BFGS改进算法，解决了BFGS每次迭代后需要保存海森矩阵的问题，只需要保存两组向量以及一组标量即可，有效地减少了对内存的占用，这对参数规模巨大的应用来说非常重要。

通过以上方法，即可<b><font color=#FF5733>求解目标函数的无约束最优化问题，并根据训练数据获得最优的特征权重</font></b>。

根据如何利用训练数据训练模型的不同方式，又可以将数值优化方法分为三类：
- 在线学习(Online Learning)，在训练模型过程中通过序列化方式以一次学习一个训练实例的方式完成，这样可以满足模型不断增量更新的需求场合。
- 批学习(Batch Learning)，每次需要所有的训练数据才能得到模型。
- Mini-Batch学习，介于两者之间，通过一次学习一批训练数据的方式完成模型更新。

## 并行随机梯度下降(Parallel Stochastic Gradient Descent)

在线学习采用随机梯度下降（Stochastic Gradient Descent，简称SGD）方法，SGD是一个在机器学习领域广泛使用的参数优化方法，是梯度下降的在线学习版本。

## 批学习并行逻辑回归

# 推荐系统:矩阵分解

常用的推荐方法按照大类别划分有以下几种：

- 基于群体统计的推荐，根据用户自身的特征，将用户归属到按某些划分标准形成的群体分类中，向用户推荐该群体感兴趣的内容，比如，可以向20～30岁年龄段的女性用户推送化妆品信息。
- 基于内容的推荐，根据用户历史上浏览或者购买过的商品的内容属性来向其推荐与此内容类似的信息。
- 基于协同过滤的推荐，根据用户群体行为来向某个特定的用户推荐符合其喜好风格的信息或商品，最常用的技术包括:
    - 基于用户的KNN(User-based KNN)，简单有效，很流行。
    - 矩阵分解方法，推荐精度高等特点。
- 基于社交的推荐，从用户的社交关系中其他用户感兴趣的内容推导特定用户可能感兴趣的内容。

## 矩阵分解算法
ALS-WR算法是一种典型的矩阵分解方法，其全称为加权$\lambda$罚交替最小二乘法(Alternating-Least-Squares with Weighted-λ-Regularization)，Mahout采用该算法来构建其推荐系统。