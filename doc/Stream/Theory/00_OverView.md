在大多数场合，流式计算对大数据处理的计算时效性要求很高，要求计算在非常短时间延时内完成，这样可以更好发挥流式计算系统的威力。流式计算的概念已经存在很长的时期，将早期和当前的流式计算系统分别称为“连续查询处理类”和“可扩展流数据平台类”计算系统。

1. 连续查询处理往往是数据流管理系统(DSMS)必须要实现的功能，一般用户输入SQL查询语句后，数据流按照时间先后顺序被切割成数据窗口，DSMS在连续流动的数据窗口中执行用户提交的SQL语句，并实时返回查询结果。包括STREAM、StreamBase、Borealis、Aurora、Telegraph等，这类系统往往会为用户提供SQL查询接口来对流数据进行挖掘。
2. “可扩展数据流平台类”计算系统与此不同，其设计初衷是出于模仿MapReduce计算框架的思路，即在对处理时效性有高要求的计算场景下，如何提供一个完善的计算框架，并暴露给用户少量的编程接口，使得用户能够集中精力处理应用逻辑。至于系统性能、低延迟、数据不丢失以及容错等问题，则由计算框架来负责，这样能够大大增加应用开发的生产力。包含Yahoo的S4和Twitter的Storm系统。

一个优秀的流计算系统应具备如下的特点：

1. 记录处理低延迟。
2. 极佳的系统容错性。
3. 极强的系统扩展能力。系统可扩展性一般指当系统计算负载过高或者存储计算资源不足以应付手头的任务时，能够通过增加机器等水平扩展方式便捷地解决这些问题。
4. 灵活强大的应用逻辑表达能力。

# DAG(有向无环图)

从流式计算的计算任务拓扑结构角度来看，一般的流式计算任务都是由计算节点和流式数据组成的DAG。DAG中的顶点是算子，表示计算，边表示数据依赖关系。DAG的节点一般是完成一个计算任务所需要的各种处理功能，比如过滤、数值累加、Join等具体计算功能，而流经各个计算节点的实时数据流构成了DAG的有向边。

## 

# 计算系统架构

常见的流式计算系统架构分为两种：主从模式（Master-Slave）和P2P模式。大多数系统架构遵循主从模式，主要是因为主控节点做全局管理比较简洁，比如Storm、MillWheel和Samza，P2P架构无中心控制节点，S4是P2P架构。其中，Samza是利用消息系统Kafka和Hadoop 2.0的资源管理系统YARN综合而成的，可以理解为是在YARN平台之上的一个应用计算框架，从本质上讲，也遵循主从架构。

## 主从架构

Storm中有主控节点和工作节点，主控节点上运行Nimbus，其主要职责是分发计算代码、在机器间分配计算任务以及故障检测等管理功能；主控节点上运行Nimbus，其主要职责是分发计算代码、在机器间分配计算任务以及故障检测等管理功能。

ZooKeeper集群用来协调Nimbus和Supervisor之间的工作，Storm将两者的状态信息存储在ZooKeeper集群上，这样Nimbus和Supervisor都成为无状态的服务节点，可以方便地进行故障恢复，无论哪个构件发生故障，都可以随时在另外一台机器上快速重新启动而不会丢失任何状态信息，但是，具体的DAG流式计算任务的计算节点可能是有状态的。

<b><font color="orange">zk上需要存储那些信息？</font></b>



## P2P架构

S4采用了P2P架构，没有中心控制节点，集群中的每台机器既负责任务计算，同时也做一部分系统管理工作，每个节点功能对等，这样的好处是系统可扩展性和容错性能好，不会产生主从模式中的单点失效问题，但管理复杂。

PE（Processing Element）是基本计算单元，属于DAG任务的计算节点，其接收到数据后触发用户应用逻辑对数据进行处理，并可能产生送向下游计算节点的衍生数据。PN（Processing Node）是PE运行的逻辑宿主（物理主机与逻辑宿主存在一对多关系），其中的事件监听器负责监听管理消息和应用数据，PEC调用对应的PE执行应用逻辑，分发器在通信层的帮助下分发数据，发送器负责对外产生衍生数据。通信层主要负责集群管理、自动容错以及逻辑宿主到物理节点的映射等功能，其可以自动侦测硬件故障，并做故障切换以及修正逻辑宿主和物理节点映射表。通信层利用ZooKeeper来协助管理P2P集群。

<center>
    <img src="./img/S4_Arch.png">
</center>



S4有一个比较严重的问题是没有合理的应用状态持久化策略，当机器出现故障时，可能存在应用状态信息丢失的问题。



## Smaza架构

<center>
    <img src="./img/Smaza_Arch.png">
</center>

Smaza是在Kafka和YARN之上封装了流式计算语义API的系统，其中，Kafka负责数据流的存储与管理，YARN负责资源管理、系统执行调度和系统容错等功能，Samza API则提供了描述执行流式计算DAG任务的接口。



Samza的任务执行流程:

<center>
    <img src="./img/Smaza_Runtime.png">
</center>



1. 通过YARN客户端向资源管理器(RM)提交任务。
2. RM从节点管理器(NM)分配计算容器给Samza的应用管理器(AM)，计算容器包含了计算所需内存、CPU等各种资源。
3. 资源分配成功，YARN在容器内启动Samza AM，Samza AM起到类似于Hadoop 1.0中JobTracker的功能，负责具体计算任务的管理协调等功能。
4. Samza AM向RM申请一个或者多个容器启动Samza任务运行器(Task Runner)，任务运行器执行用户编码的应用逻辑，其对应的输入流和输出流都通过Kafka Broker来进行管理。

这样，一个Samza流式计算任务就可以启动起来并执行。

# 任务故障

<b><font color="orange">对于长期运行的流式作业，每个任务都谁是会出现故障，如何确保能够透明地处理这些故障，让流式作业可以继续运行。要实现这一点需要保证任务故障时可以继续运行，还需要保证结果和算子状态的正确性。</font></b>

对于流处理中的每个事件，任务都要执行以下几步：①接收事件并将它们存在本地缓冲区；②选择性更新内部状态；③产生输出记录。以上每步都可能发生故障，系统必须在故障情况下明确定义其行为。如果故障发生在第一步，事件是否会丢失？如果更新内部状态后发生故障，系统恢复后是否会重复更新？上述情况下，结果是否正确？



## 结果保障

结果保障指的是流处理引擎内部状态的一致性，也就是关注故障恢复后应用代码能够看到的状态。需要注意的是，保证应用状态的一致性和保证输出的一致性不是一回事。

---

**至多一次(At-Most Once Delivery)**

它保证每个事件至多被处理一次，也就是说没有机制来保证结果的正确性。上游节点不能保证消息被送达到下游节点。如果计算系统容错机制不完善，存在丢数据的可能性。`S4`和`MUDP8`属于这种类型。

---

 **至少一次(At-Least Once Delivery)**

对于大多数应用，用户不希望丢数据，这类保证称为至少一次。上游节点保证向下游节点送达一次或者多次相同的数据。所欲的数据都会被处理，但是有些数据可能会被处理多次。如果正确性仅依赖于信息的完整性，重复处理可以接受。例如，确定某个事件是否在输入流中出现过，可以采用至少一次保障正确实现；但如果要计算某个事件在输入流中出现的次数，至少一次保障会导致错误的计算结果。

为保证至少一次结果语义的正确性，需要在源头或缓冲区中去重。持久化时间日志会将所有的事件写入永久存储，这样可以在任务故障恢复时重放他们。一种实现方式是采用记录确认(record-acknowledgments)，它会将所有的数据都存在缓冲区中，直到处理管道中所有的任务都确认某个事件已经处理完毕才将数据丢弃。

---

**精确一次(Exact-Once Delivery)**

上游节点保证将流数据正确地送达下游节点且只正确送达一次。它表示没有数据的丢失，而且每个事件对于内部状态的更新都只有一次。本质上，精确一次保证意味着应用总会提供正确的结果如同故障没有发生过一样。`Storm`属于这种，它借助送达保证机制(实现至少送达一次)+事务拓扑(保证不会出现多次送达)联合完成。`Flink`也属于这种，它借助轻量级检查点机制来实现。

精确一致性保障以一次性保障为前提，同样需要数据重放，此外还需要流处理引擎保证内部状态的一致性，即在故障恢复后，引擎需要知道某个时间对应的更新是否已经反映在状态上。

---

**端到端的精确一次**

上面三个都属于应用状态的一致性。它在实际流处理中，处理流处理引擎还需要数据源组件和数据终点组件，端到端的精确一次指的是在整个数据管道上结果都是正确的。在每个组件都提供自身保证的情况下，整个数据管道上端到端保障受制于保障最弱的组件。在某些时候，可以借助弱保障来实现强语义，常见情况是求最大值或最小值的幂等操作，可以用至少一次保证来实现精确一次语义。

---

# 状态持久化

流式计算系统中常见的系统容错有三种模式：备用服务、热备和检查点机制。

## 备用服务(Standby Service)

计算任务的某个计算节点N在另外一台物理机上设置其对应的备份服务S，计算框架定时通过心跳或者ZooKeeper等来及时捕获服务状态，当节点N发生故障时，启动备份服务S来接替计算节点N的功能。这只适合计算节点属于无状态（Stateless）类型的服务，因为一旦计算节点N死掉，如果存有状态信息，则状态信息全部丢失，无法在计算节点S进行状态恢复，存在状态丢失的可能性。这对于流式计算系统来说是很严重的功能不足，大大限制了其使用场景。

<center>
    <img src="./img/Standby-Service.png">
</center>



## 热备服务(Host Standby)

热备机制可以避免备用服务机制的不足。与备份服务不同的是，热备机制的计算节点N和其备用节点S同时运行相同的功能，上游节点将数据流同时发往下游的计算节点N及其备用节点S，当计算节点N发生故障时对系统无任何影响，因为备用节点S一直和节点N同时运行，所以即使是有状态的服务，两者也时刻保持着相同的状态信息。其好处是显而易见的，但是也有对应的缺点：一个是备用节点额外耗费各种系统资源。另外，正常运行时，在两个节点的下游需要有“**流选择器”来保证只有一个上游数据能够通过，避免数据重复**。

<center>
    <img src="./img/Host-Standby-Service.png">
</center>

## 检查点(Checkpointing)

其与“备用服务”架构基本相同，但是两者的不同点在于：为了能够在故障替换时恢复计算节点N的状态信息，计算节点N周期性地将其状态信息通过检查点的方式在其他地方进行备份，当计算框架侦测到计算节点N发生故障时，则启动备用节点S，并从Log中将对应的状态信息进行恢复，这样，即使对有状态的服务也可以保证正常切换。

<center>
    <img src="./img/Checkpointing.png">
</center>

检查点方式也有其对应的两个缺点。首先，如果状态信息较多，为了恢复状态信息，备用节点切换过程可能较长。其次，检查点备份的时间周期也需要仔细斟酌，如果备份周期长，则很可能在上次和下次备份信息之间的系统发生故障，这样依然会存在丢失状态信息的可能；而如果备份周期短，则系统会花费很多资源用在数据备份上，也会影响系统整体的性能。




| 流计算系统 |      | 系统架构 | 保证送达                                                  | 状态持久化                                        |
| ---------- | ---- | -------- | --------------------------------------------------------- | ------------------------------------------------- |
| Flink      |      |          | 恰好一次送达                                              | 检查点                                            |
| Storm      |      | 主从模式 | 恰好一次送达<br/>(通过送达保证机制和事务拓扑结构联合完成) | 检查点                                            |
| MillWheel  |      | 主从模式 | 恰好一次送达<br/>                                         | 检查点<br/>(为加快性能，支持强方式和弱方式持久化) |
| Samza      |      | 主从模式 | 至少一次送达                                              | 检查点                                            |
| S4         |      | P2P模式  |                                                           | 备用服务<br/>(可能丢失状态信息)                   |

# 参考资料

1. 大数据日知录
2. 

